# -*- coding: utf-8 -*-
"""Citizen AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmk3Hlc8wzuSaslyr4Znfrc_2bl_Zfur
"""

!pip install transformers torch gradio -q

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# Ensure pad token exists
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token


def generate_response(prompt, max_length=800, temperature=0.7):
    if not prompt.strip():
        return "‚ö†Ô∏è Please enter a valid input."

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()


def city_analysis(city_name, max_length, temperature):
    prompt = (
        f"Provide a detailed analysis of {city_name} including:\n"
        "1. Crime Index and safety statistics\n"
        "2. Accident rates and traffic safety information\n"
        "3. Overall safety assessment\n\n"
        f"City: {city_name}\nAnalysis:"
    )
    return generate_response(prompt, max_length=max_length, temperature=temperature)


def citizen_interaction(query, max_length, temperature):
    prompt = (
        "As a government assistant, provide accurate and helpful information "
        "about the following citizen query related to public services, government policies, or civic issues:\n\n"
        f"Query: {query}\nResponse:"
    )
    return generate_response(prompt, max_length=max_length, temperature=temperature)


# Build Gradio app
with gr.Blocks() as app:
    gr.Markdown("# üåÜ City Analysis & üèõ Citizen Services AI")

    with gr.Accordion("‚öôÔ∏è Generation Settings", open=False):
        max_length = gr.Slider(200, 1500, value=800, step=50, label="Max Response Length")
        temperature = gr.Slider(0.1, 1.5, value=0.7, step=0.1, label="Creativity (Temperature)")

    with gr.Tabs():
        with gr.TabItem("City Analysis"):
            city_input = gr.Textbox(
                label="Enter City Name",
                placeholder="e.g., New York, London, Mumbai...",
                lines=1
            )
            city_output = gr.Textbox(label="City Analysis (Crime & Safety)", lines=15)
            analyze_btn = gr.Button("üîç Analyze City")
            analyze_btn.click(city_analysis, inputs=[city_input, max_length, temperature], outputs=city_output)

        with gr.TabItem("Citizen Services"):
            citizen_query = gr.Textbox(
                label="Your Query",
                placeholder="Ask about public services, government policies, civic issues...",
                lines=4
            )
            citizen_output = gr.Textbox(label="Government Response", lines=15)
            query_btn = gr.Button("üìå Get Information")
            query_btn.click(citizen_interaction, inputs=[citizen_query, max_length, temperature], outputs=citizen_output)

app.launch(share=True)